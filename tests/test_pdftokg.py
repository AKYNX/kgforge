from kgforge import PdfToKg
from kgforge.utils import TextLoader
from kgforge.documents import Document


def test_answer_question() -> None:
    sample_obj = PdfToKg()
    question = "What is the main contribution of this research paper - a method?, or an analysis? or results? or something else?"
    context = " predicting research that will be cited in policy documents bharat kale northern illinois university bkale@niu.eduharish varma siravuri northern illinois university hsiravuri@niu.edu hamed alhoori northern illinois university alhoori@niu.edumichael e. papka argonne national laboratory northern illinois university papka@niu.edu abstract scientific publications and other genres of research output are in- creasingly being cited in policy documents. citations in documents of this nature could be considered a critical indicator of the signifi- cance and societal impact of the research output. in this study, we built classification models that predict whether a particular research work is likely to be cited in a public policy document based on the attention it received online, primarily on social media platforms. we evaluated the classifiers based on their accuracy, precision, and recall values. we found that random forest and multinomial naive bayes classifiers performed better overall. keywords public policy, policy documents, altmetrics, social media 1 introduction policy documents influence large sections of society [ 4]. because of the unique importance of policy documents across diverse orga- nizations, citations included in this type of material support both the credibility of the author cited and the credibility of the policy document itself [ 6]. likewise, in this context, it may be appropriate to assign a policy document citation more weight than a regular citation included in a literature review in a scholarly paper, for example. haunschild and bornmann [ 7] studied the percentage of papers in web of science that are mentioned in policy-related documents and found that less than 0.5% of the papers on a range of subjects had been mentioned at least once in policy-related documents. lauren [ 5] analyzed patterns in the types of altmetric attention received by papers that make it into policy documents and found that papers are often being referenced quickly, i.e., within 2 years of publication, such that they are having a real-world impact sooner than expected winterfeldt [ 12] presented a framework to bridge the gap be- tween science and decision making in the policy sphere. orduna- malea, thelwall, and kousha [7] explored the relationship between permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for third-party components of this work must be honored. for all other uses, contact the owner/author(s). websci ’17, june 25-28, 2017, troy, ny, usa ©2017 copyright held by the owner/author(s). acm isbn 978-1-4503-4896-6/17/06. https://doi.org/10.1145/3091478.3098865citations in patents and technological impact and found that the number of patents citing a resource indicates the technological capacity or relevance of that resource. according to black [ 3], al- though evidence-based policy-making is being encouraged in all areas of public service, research is currently under-used in policy- making and there is a need for a better mutual understanding between research and policy communities. citation analysis is self-limiting because it does not account for many other signals through which research receives attention. an increasing amount of scholarly content is being shared and dis- cussed daily on social media platforms [ 1]. whereas citations mea- sure research impact within scholarly boundaries, non-traditional web-based metrics or altmetrics [ 8][2] make it possible to mea- sure different influences, including readers who read an article or share, and/or discuss it with others, but do not formally cite it in traditionally published articles. thelwall et al. [ 11] studied the potential value of altmetrics for evaluating funding criteria and found that some metrics can be helpful in this sphere. sarewitz and pielke [ 10] proposed a method to strengthen the connection between science policy decisions, sci- entific research, and social outcomes using the example of climate change research. pawson [ 9] discussed various ways to incorporate research results into the policy-making process. to date, most stud- ies focus on understanding and using altmetrics in reference to only a few measures. the present study is the first to explore modeling altmetrics in order to predict citations in policy documents. 2 data collection the dataset in this study is a database dump that we obtained from altmetric.com, which consists of 5.2 million articles. our initial analysis showed that of these articles, 89,350 had been cited in at least one policy document whereas 5,097,207 had not been included in a document of this kind. to create a balanced dataset for further analysis, along with the 89,350 articles that had been cited in a policy document, we randomly chose another 89,350 articles that had not been cited in a policy document. the result was a balanced dataset with approximately 180,000 records, half of which had been cited in policy documents. 3 feature selection the dataset has a very rich set of features for each article. however, in our analysis, we considered only features related to online at- tention. the dataset consists of mention counts on various online sources including reference managers, mainstream news outlets, poster websci '17, june 25-28, 2017, troy, ny, usa 389  blogs, peer-review platforms (e.g., pubpeer and publons), social media, public policy documents, and wikipedia. we used mention counts on twitter, facebook, reddit, mendeley, google+, wikipedia, weibo, mainstream news outlets, blogs, videos, and peer review sites as features to build the classifiers. yet, we left a few sources out of our account, including connotea, which was discontinued in 2013, and pinterest and stackoverflow, which together contributed to less than 1% of the articles in the sample. we replaced the policy citation count with a binary class label denoting whether a given article had been cited in a policy document. 4 methods and results 4.1 classification to predict the likelihood of a research article being cited in a policy document, we implemented three classifiers: the multinomial naive bayes classifier, the random forest classifier with the number of trees set at 100, and a c-support vector machine with the radial basis function (rbf) kernel. we then divided the entire dataset into training and test sets comprising 80% and 20% of the entire dataset, respectively. we trained the models using a 10-fold cross-validation technique and evaluated them based on accuracy, precision, recall, and f1-measure metrics, as shown in table 1. table 1: accuracy, precision, recall, and f1-measure for dif- ferent models multinomial naive bayes random forest svm accuracy 0.842 0.870 0.868 precision 0.802 0.826 0.820 recall 0.905 0.870 0.868 f1-measure 0.850 0.844 0.824 4.2 feature ranking with the classification models built, we calculated the weight for each feature to determine the significance of each in making the final prediction. given that feature weights in the case of a sup- port vector machine can be determined only for linear kernels, we ranked the features based on their relevance for only the random forest and multinomial naive bayes classifiers. we ranked the fea- tures in regard to their importance to the random forest classifier from most to least important, as shown in table 2. 5 conclusions and future work in this study, we used a specific set of features that track online attention received by scholarly articles to build classifiers to pre- dict the likelihood of an article being cited in public policy. the random forest classifier showed better results in making predic- tions. we found mention counts on peer-review platforms to be the most influential feature, whereas news rated as the least influential feature. the promising results obtained in this work show that a re- lationship exists between the online attention that a scholarly work receives and the policy citations it generates, which we were able to exploit. we intend to extend our work in this area by building regression models to predict the number of policy citations a giventable 2: feature ranking for different models platform random forest multinomial naive bayes peer-review 0.273595 4.4267 google+ 0.197488 3.4210 reddit 0.151016 4.4087 video 0.098035 4.9458 twitter 0.068745 2.2421 weibo 0.088242 3.7988 mendeley 0.030116 0.3210 wikipedia 0.026027 4.9668 blogs 0.018631 4.4571 facebook 0.016189 3.2314 news 0.008926 3.7307 work is likely to receive. we also plan to build more classifiers with different feature sets and to compare our results. acknowledgments mep was supported in part by the office of advanced scientific computing research, office of science, u.s. department of energy, under contract de-ac02-06ch11357. references [1] euan adie and william roe. 2013. altmetric: enriching scholarly content with article-level discussion and metrics. learned publishing 26, 1 (2013), 11–17. https://doi.org/10.1087/20130103 [2]hamed alhoori and richard furuta. 2014. do altmetrics follow the crowd or does the crowd follow altmetrics?. in proceedings of the 14th acm/ieee-cs joint conference on digital libraries. 375–378. https://doi.org/10.1109/jcdl.2014. 6970193 [3] nick black and anna donald. 2001. evidence based policy: proceed with carecom- mentary: research must be taken seriously. bmj 323, 7307 (2001), 275–279. https://doi.org/10.1136/bmj.323.7307.275 [4] lutz bornmann, robin haunschild, and werner marx. 2016. policy documents as sources for measuring societal impact: how often is climate change research mentioned in policy-related documents? scientometrics 109, 3 (2016), 1477–1495. https://doi.org/10.1007/s11192-016-2115-y [5] lauren cadwallader. 2016. papers, policy documents and patterns of attention. in3:am the altmetrics conference. https://doi.org/10.17863/cam.4844 [6] richard freeman and jo maybin. 2011. documents, practices and policy. evidence & policy: a journal of research, debate and practice 7, 2 (2011), 155–170. [7]robin haunschild and lutz bornmann. 2017. how many scientific papers are mentioned in policy-related documents? an empirical investigation using web of science and altmetric data. scientometrics 110, 3 (2017), 1209–1216. https: //doi.org/10.1007/s11192-016-2237-2 [8] kim johan holmberg. 2015. altmetrics for information professionals: past, present and future. elsevier. https://books.google.com/books?id=ghdibqaaqbaj [9] ray pawson. 2002. evidence-based policy: in search of a method. evaluation 8, 2 (2002), 157–181. https://doi.org/10.1177/1358902002008002512 [10] daniel sarewitz and roger a pielke. 2007. the neglected heart of science policy: reconciling supply of and demand for science. environmental science & policy 10, 1 (2007), 5–16. https://doi.org/10.1016/j.envsci.2006.10.001 [11] mike thelwall, kayvan kousha, adam dinsmore, and kevin dolby. 2016. alterna- tive metric indicators for funding scheme evaluations. aslib journal of informa- tion management 68, 1 (2016), 2–18. https://doi.org/10.1108/ajim-09-2015-0146 [12] detlof von winterfeldt. 2013. bridging the gap between science and decision making. proceedings of the national academy of sciences 110, supplement 3 (2013), 14055–14061. https://doi.org/10.1073/pnas.1213532110 poster websci '17, june 25-28, 2017, troy, ny, usa 390"
    resp = sample_obj.answer_question(question=question, context=context)
    print(resp)
    assert True


def test_read_pdf() -> None:
    sample_obj = TextLoader()
    path = "tests/test_data/example.pdf"
    resp = sample_obj._read_pdf(path=path)
    assert len(resp) > 0


def test_document() -> None:
    sample_obj = Document()
    sample_obj.full_text = "sample-text"
    question = "text"
    resp = sample_obj.answer_question(question=question)
    assert len(resp) >= 0
